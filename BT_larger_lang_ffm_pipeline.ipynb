{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "functional-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "from datetime import datetime, timedelta\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contemporary-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitrr/miniconda3/envs/py3.8/lib/python3.8/site-packages/google/auth/_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project=\"sharechat-production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collectible-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = [\n",
    "    \"Hindi\",\n",
    "    # \"Tamil\",\n",
    "    # \"Telugu\",\n",
    "#     \"Kannada\",\n",
    "#     \"Punjabi\",\n",
    "#     \"Odia\",\n",
    "#     \"Bengali\",\n",
    "#     \"Marathi\",\n",
    "#     \"Malayalam\",\n",
    "#     \"Gujarati\",\n",
    "]\n",
    "# DAYS_OF_DATA_CONSIDERED = 7\n",
    "TRAINING_DAYS = 30\n",
    "TESTING_DAYS = 3\n",
    "rating_def_dict = {\n",
    "#         \"vplay\": \"is_vp_succ\",\n",
    "#         \"like\": \"is_like\",\n",
    "#         \"share\": \"is_share\",\n",
    "#         \"fav\": \"is_fav\",\n",
    "        \"vplay_skip\": \"is_vp_skip\",\n",
    "        \"vplay2\": \"is_vp_succ2\",\n",
    "    }\n",
    "BASE_BIG_QUERY_PATH = \"maximal-furnace-783.rohitrr\"\n",
    "RANDOM_SEED = 9745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "twelve-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSqlFile(file_path, lang, rating_def = \"\", q0_table = \"\", \n",
    "                q1_table = \"\", q2_table = \"\", end_time = \"\", days = 2,\n",
    "               common_posts_end_time = \"\", common_posts_days = 2):\n",
    "    with open (file_path, \"r\") as file:\n",
    "        sql_command=file.read()\n",
    "        sql_command = sql_command.format(\n",
    "            common_posts_end_time = common_posts_end_time if common_posts_end_time == \"\" else \\\n",
    "                                    str(common_posts_end_time.strftime('%Y-%m-%d %H:%M:%S')),\n",
    "            common_posts_days = common_posts_days,\n",
    "            days=days,\n",
    "            end_time=end_time if end_time == \"\" else \\\n",
    "                        str(end_time.strftime('%Y-%m-%d %H:%M:%S')),\n",
    "            language=lang,\n",
    "            rating_def=rating_def,\n",
    "            q0table=q0_table,\n",
    "            q1table=q1_table,\n",
    "            q2table=q2_table\n",
    "        )\n",
    "    return sql_command\n",
    "\n",
    "def delete_tables(delete_tables_path_list):\n",
    "    for delete_table_path in delete_tables_path_list:\n",
    "        client.delete_table(delete_table_path)\n",
    "\n",
    "    print(\"All tables deleted\")\n",
    "\n",
    "def download_large_table_to_gcs(table_path, bucket_name,\n",
    "                              format_suffix = \".csv\"):\n",
    "    '''\n",
    "        table_path - Example is \"maxinal-furnace-783.rohitrr.sample_test_table\"\n",
    "        gcs_file_name - Should be of the form file_name/*.csv in case\n",
    "                        the table is expected to be larger than 1GB\n",
    "    \n",
    "    Output -  A folder name with the same table name (and extra suffixes) which is \n",
    "    created in the gcs bucket. The table is partitioned and stored in the folder\n",
    "    '''\n",
    "    \n",
    "    print(\"Downloading table - {} to gcs\".format(table_path))\n",
    "    temp = table_path.split('.')\n",
    "    project = temp[0]\n",
    "    dataset_id = temp[1]\n",
    "    table_id = gcs_folder_name = temp[2]\n",
    "    \n",
    "#     In order to to ensure files don't get simply added to exisitng folders unique suffixes are generated\n",
    "    unique_suffix = datetime.utcnow().strftime('%Y-%m-%d_%H:%M:%S') +\\\n",
    "                    '_' + str(randint(0, 200))\n",
    "    gcs_folder_name = gcs_folder_name + '_' + unique_suffix\n",
    "\n",
    "    \n",
    "#     As the table is large gcs file name must be a wild card for google \n",
    "#     cloud to partition and download the table\n",
    "    gcs_file_name = gcs_folder_name + \"/*\"+ format_suffix\n",
    "#     gcs_file_name will be like file_name/*.csv\n",
    "\n",
    "    destination_uri = \"gs://{}/{}\".format(bucket_name, gcs_file_name)\n",
    "    dataset_ref = bigquery.DatasetReference(project, dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    \n",
    "    job_config = bigquery.ExtractJobConfig(field_delimiter=\"\\t\")\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref,\n",
    "        destination_uri,\n",
    "        job_config = job_config,\n",
    "        # Location must match that of the source table.\n",
    "        location=\"US\",\n",
    "    )  # API request\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "    print(\n",
    "        \"Exported {}:{}.{} to {}\".format(project, dataset_id, table_id, destination_uri)\n",
    "    )\n",
    "    return gcs_folder_name\n",
    "    \n",
    "def download_files_from_folder_in_gcs(bucket_name, gcs_folder_name, dest_folder_path):\n",
    "    print(\"Downloading from gcs_folder_name {} to local\".\n",
    "          format(gcs_folder_name))\n",
    "    \n",
    "    pathlib.Path(dest_folder_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    storage_client = storage.Client(project=\"maximal-furncace-783\")\n",
    "    bucket = storage_client.get_bucket(bucket_or_name=bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=gcs_folder_name)  # Get list of files\n",
    "    for blob in blobs:\n",
    "        filename = blob.name.replace('/', '_') \n",
    "        blob.download_to_filename(os.path.join(dest_folder_path,\n",
    "                                              filename))\n",
    "\n",
    "    \n",
    "    print(f\"Contents in gs://{bucket_name}/{gcs_folder_name} \\\n",
    "    transferred to {dest_folder_path}\")\n",
    "\n",
    "def merge_and_save_csv_files(csv_files_folder_path, dest_folder_path, with_common_header = True,\n",
    "                             out_file_name = \"merged_out.txt\"):\n",
    "    print(f\"Merging and saving files from {csv_files_folder_path} to {dest_folder_path}\")\n",
    "    \n",
    "    pathlib.Path(dest_folder_path).mkdir(parents = True, exist_ok = True)\n",
    "    csv_file_names = [file_name for file_name \\\n",
    "                  in os.listdir(csv_files_folder_path)\\\n",
    "                 if file_name.endswith(\".csv\")]\n",
    "    \n",
    "    f_out = open(os.path.join(dest_folder_path, out_file_name), 'w')\n",
    "    for csv_file_name in csv_file_names:\n",
    "        csv_file_path = os.path.join(csv_files_folder_path, csv_file_name)\n",
    "        with open(csv_file_path) as f_csv_in:\n",
    "            header = next(f_csv_in)\n",
    "            if(with_common_header):\n",
    "                f_out.write(header)\n",
    "                with_common_header = False # After writing header once, do not write again\n",
    "                \n",
    "            for line in f_csv_in:\n",
    "                f_out.write(line)\n",
    "    f_out.close()\n",
    "    print(f\"Saved file {out_file_name} in {dest_folder_path}\")\n",
    "    \n",
    "def download_table_to_local_as_one_file(table_path, local_save_path, out_file_name = \"fetched_table.csv\",\n",
    "                                        with_header = True, bucket_name = \"query_runner_results\"):\n",
    "    local_download_folder_path = os.path.join(local_save_path, \"temp_download_folder\")\n",
    "    if(os.path.exists(local_download_folder_path)):\n",
    "        print(f\"Temporary download folder - {local_download_folder_path} \\\n",
    "              already present - removing it to avoid using older files\")\n",
    "        shutil.rmtree(local_download_folder_path)\n",
    "        print(\"Old temp folder removed\")\n",
    "        \n",
    "    merged_out_path = local_save_path\n",
    "    \n",
    "    downloaded_gcs_folder_name = download_large_table_to_gcs(table_path, bucket_name)\n",
    "    download_files_from_folder_in_gcs(bucket_name, downloaded_gcs_folder_name,\n",
    "                                     local_download_folder_path)\n",
    "    merge_and_save_csv_files(local_download_folder_path, merged_out_path, with_common_header=with_header,\n",
    "                            out_file_name=out_file_name)\n",
    "    \n",
    "    shutil.rmtree(local_download_folder_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enormous-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_base_table(lang, common_posts_end_time, \n",
    "                         common_posts_days, end_time, days,\n",
    "                         overwrite_base_table = False, \n",
    "                         mode = \"train\"):\n",
    "    \n",
    "    temp_q0_table_path = BASE_BIG_QUERY_PATH+'.'+f'{mode}_temp_q0_table_{lang}'\n",
    "    if(not overwrite_base_table):\n",
    "        try:\n",
    "            client.get_table(temp_q0_table_path)\n",
    "            print(f\"Table-{temp_q0_table_path} already exists, not overwriting\")\n",
    "            return temp_q0_table_path\n",
    "        except NotFound:\n",
    "            print(f\"Table-{temp_q0_table_path} not already present - going ahead creating it\")\n",
    "            \n",
    "    print(f\"Running query 1 for {lang} .....\")\n",
    "    job_config = bigquery.QueryJobConfig(destination= temp_q0_table_path,\n",
    "                                         write_disposition = \"WRITE_TRUNCATE\"\n",
    "                                         )\n",
    "    sql = readSqlFile(\"./queries/video/query0.sql\", lang = lang,\n",
    "                      common_posts_end_time = common_posts_end_time,\n",
    "                      common_posts_days = common_posts_days,\n",
    "                      end_time=end_time, days = days)\n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "    query_job.result()\n",
    "    print(f\"Query 0 results loaded to the table {temp_q0_table_path}\")\n",
    "    return temp_q0_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daily-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_prepare_data_with_base_table(lang, rating_def, base_q0_table_path,\n",
    "                                             end_time, days,\n",
    "                                             save_path,\n",
    "                                             table_with_mapping = None,\n",
    "                                             mode=\"train\"):\n",
    "    delete_tables = []\n",
    "    # Run Q1 query\n",
    "    temp_q0_table_path = base_q0_table_path\n",
    "    temp_q1_table_path = BASE_BIG_QUERY_PATH+'.'+f'{mode}_temp_q1_table_{lang}_{rating_def}'\n",
    "    job_config = bigquery.QueryJobConfig(destination= temp_q1_table_path,\n",
    "                                         write_disposition = \"WRITE_TRUNCATE\"\n",
    "                                         )\n",
    "    sql = readSqlFile(\"./queries/video/query1.sql\", lang = lang, \n",
    "                      rating_def = rating_def, end_time=end_time, \n",
    "                      days = days,\n",
    "                     q0_table = temp_q0_table_path)\n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "    query_job.result()\n",
    "    print(f\"Query 1 results loaded to the table {temp_q1_table_path}\")\n",
    "    delete_tables.append(temp_q1_table_path)\n",
    "\n",
    "    # Run Q2 query\n",
    "    if(mode == \"train\" and table_with_mapping == None):\n",
    "        table_with_mapping = BASE_BIG_QUERY_PATH+'.'+f'{mode}_temp_q2_table_{lang}_{rating_def}'\n",
    "        job_config = bigquery.QueryJobConfig(destination= table_with_mapping, \n",
    "                                             write_disposition = \"WRITE_TRUNCATE\")\n",
    "        sql = readSqlFile(\"./queries/video/query2.sql\", lang = lang, \n",
    "                          rating_def = rating_def, end_time=end_time, days = days,\n",
    "                         q1_table = temp_q1_table_path)\n",
    "        query_job = client.query(sql, job_config=job_config)\n",
    "        query_job.result()\n",
    "        print(f\"Query 2 results loaded to the table {table_with_mapping}\")\n",
    "        download_table_to_local_as_one_file(table_with_mapping, save_path, \n",
    "                                    out_file_name = f\"user_post_ffm_mapping.csv\")\n",
    "        delete_tables.append(table_with_mapping)\n",
    "\n",
    "    # Run Q3 query\n",
    "    temp_q3_table_path = BASE_BIG_QUERY_PATH+'.'+f'{mode}_temp_q3_table_{lang}_{rating_def}'\n",
    "    job_config = bigquery.QueryJobConfig(destination= temp_q3_table_path,\n",
    "                                         write_disposition = \"WRITE_TRUNCATE\"\n",
    "                                         )\n",
    "    sql = readSqlFile(\"./queries/video/query3.sql\", lang = lang, \n",
    "                      rating_def = rating_def, end_time=end_time, days = days,\n",
    "                      q1_table = temp_q1_table_path,\n",
    "                     q2_table = table_with_mapping)\n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "    query_job.result()\n",
    "    print(f\"Query 3 results loaded to the table {temp_q3_table_path}\")\n",
    "    delete_tables.append(temp_q3_table_path)\n",
    "\n",
    "#     Save results to local storage\n",
    "    download_table_to_local_as_one_file(temp_q3_table_path, save_path, with_header=False,\n",
    "                                        out_file_name = f\"{mode}.txt\")\n",
    "    return delete_tables, table_with_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "retained-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-30 00:00:00 2021-03-27 00:00:00\n"
     ]
    }
   ],
   "source": [
    "common_end_time = datetime(2021, 4, 30) # the hours, minutes and seconds are taken to be 0\n",
    "test_end_time = common_end_time\n",
    "train_end_time = common_end_time - timedelta(TESTING_DAYS)\n",
    "print(test_end_time, train_end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-stable",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table-maximal-furnace-783.rohitrr.train_temp_q0_table_Hindi already exists, not overwriting\n",
      "Table-maximal-furnace-783.rohitrr.test_temp_q0_table_Hindi already exists, not overwriting\n",
      "Query 1 results loaded to the table maximal-furnace-783.rohitrr.train_temp_q1_table_Hindi_is_vp_skip\n",
      "Query 2 results loaded to the table maximal-furnace-783.rohitrr.train_temp_q2_table_Hindi_is_vp_skip\n",
      "Temporary download folder - ./train_test_data_models/Hindi/is_vp_skip/temp_download_folder               already present - removing it to avoid using older files\n",
      "Old temp folder removed\n",
      "Downloading table - maximal-furnace-783.rohitrr.train_temp_q2_table_Hindi_is_vp_skip to gcs\n",
      "Exported maximal-furnace-783:rohitrr.train_temp_q2_table_Hindi_is_vp_skip to gs://query_runner_results/train_temp_q2_table_Hindi_is_vp_skip_2021-04-06_04:57:55_96/*.csv\n",
      "Downloading from gcs_folder_name train_temp_q2_table_Hindi_is_vp_skip_2021-04-06_04:57:55_96 to local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitrr/miniconda3/envs/py3.8/lib/python3.8/site-packages/google/auth/_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents in gs://query_runner_results/train_temp_q2_table_Hindi_is_vp_skip_2021-04-06_04:57:55_96     transferred to ./train_test_data_models/Hindi/is_vp_skip/temp_download_folder\n",
      "Merging and saving files from ./train_test_data_models/Hindi/is_vp_skip/temp_download_folder to ./train_test_data_models/Hindi/is_vp_skip\n",
      "Saved file user_post_ffm_mapping.csv in ./train_test_data_models/Hindi/is_vp_skip\n",
      "Query 3 results loaded to the table maximal-furnace-783.rohitrr.train_temp_q3_table_Hindi_is_vp_skip\n",
      "Downloading table - maximal-furnace-783.rohitrr.train_temp_q3_table_Hindi_is_vp_skip to gcs\n",
      "Exported maximal-furnace-783:rohitrr.train_temp_q3_table_Hindi_is_vp_skip to gs://query_runner_results/train_temp_q3_table_Hindi_is_vp_skip_2021-04-06_05:02:04_119/*.csv\n",
      "Downloading from gcs_folder_name train_temp_q3_table_Hindi_is_vp_skip_2021-04-06_05:02:04_119 to local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitrr/miniconda3/envs/py3.8/lib/python3.8/site-packages/google/auth/_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lang in LANGS:\n",
    "    base_q0_train_table_path = construct_base_table(lang, train_end_time, TRAINING_DAYS, \n",
    "                                                    train_end_time, TRAINING_DAYS,\n",
    "                                             overwrite_base_table=False, mode=\"train\")\n",
    "    \n",
    "    base_q0_test_table_path = construct_base_table(lang, train_end_time, TRAINING_DAYS, \n",
    "                                             test_end_time, TESTING_DAYS,\n",
    "                                             overwrite_base_table=False, mode=\"test\")\n",
    "    \n",
    "    for key, rating_def in rating_def_dict.items():\n",
    "        save_path = f\"./train_test_data_models/{lang}/{rating_def}\"\n",
    "        train_delete_table_paths, train_table_with_mapping = \\\n",
    "        collect_and_prepare_data_with_base_table(lang, rating_def,\n",
    "                                 base_q0_train_table_path,\n",
    "                                 end_time = train_end_time,\n",
    "                                 save_path = save_path,\n",
    "                                 days = TRAINING_DAYS,\n",
    "                                 mode = \"train\")\n",
    "\n",
    "        test_delete_table_paths, _ = \\\n",
    "        collect_and_prepare_data_with_base_table(lang, rating_def,\n",
    "                                 base_q0_test_table_path,\n",
    "                                 table_with_mapping = train_table_with_mapping,\n",
    "                                 end_time = test_end_time, days = TESTING_DAYS,\n",
    "                                 save_path = save_path,\n",
    "                                 mode = \"test\")\n",
    "#         Delete all created tables\n",
    "#         delete_tables(\n",
    "#             train_delete_table_paths+test_delete_table_paths\n",
    "#         )\n",
    "#         Train using xlearn binary\n",
    "        print(f\"Training started for label {rating_def} in {lang} .......\")\n",
    "        model_output_path = os.path.join(save_path, \"out\")\n",
    "        pathlib.Path(model_output_path).mkdir(parents = True, exist_ok = True)\n",
    "        cmd = f\"./xlearn_train {save_path}/train.txt \\\n",
    "        -v {save_path}/test.txt -x auc -s 2 -k 32 -m {model_output_path}/model.out \\\n",
    "        -t {model_output_path}/model.txt -b 0.001 --disk 2>&1 | tee \\\n",
    "        {model_output_path}/logs\"\n",
    "        os.system(cmd)\n",
    "        print(f\"Model trained and saved in {model_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-issue",
   "metadata": {},
   "source": [
    "### Scrap code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "exceptional-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = readSqlFile(\"./queries/video/query0.sql\", \n",
    "                  lang = \"Odia\", rating_def = rating_def, \n",
    "                  end_time=end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_table(temp_q1_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "musical-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(f\"./train_test_data/{rating_def}/{lang}\").mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sitting-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT * FROM `{temp_q3_table_path}`\n",
    "\"\"\"\n",
    "data_df = client.query(sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "heated-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size = 0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "tracked-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"./train_test_data/{rating_def}/{lang}\"\n",
    "pathlib.Path(save_path).mkdir(parents = True, exist_ok = True)\n",
    "test_df.to_csv(os.path.join(save_path, \"test.txt\"), sep=\"\\n\", header = False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "unsigned-arthur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.44 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 64 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 517707\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 2\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 23.14 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 256.74 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.34 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss            Test AUC     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m     1            0.418973            0.410742            0.769566               24.66\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m     2            0.400472            0.404218            0.779871               24.93\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m     3            0.375957            0.403592            0.782683               23.41\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m     4            0.334905            0.418434            0.769949               23.34\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m     5            0.295719            0.439514            0.754984               24.52\n",
      "\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m     6            0.267937            0.459180            0.743783               23.32\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Early-stopping at epoch 3, best AUC: 0.782683\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: out/model.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.21 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save txt model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mTXT Model file: out/model.txt\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving txt model: 13.31 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mTotal time cost: 181.45 (sec)\n"
     ]
    }
   ],
   "source": [
    "! ./xlearn_train ./train_test_data/is_vp_succ2/Odia/train.txt -v ./train_test_data/is_vp_succ2/Odia/test.txt -x auc -s 2 -k 32 -m out/model.out -t out/model.txt -b 0.001 --disk 2>&1 | tee out/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "varying-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"./xlearn_train {save_path}/train.txt \\\n",
    "-v {save_path}/test.txt -x auc -s 2 -k 32 -m {model_output_path}/model.out \\\n",
    "-t {model_output_path}/model.txt -b 0.001 --disk 2>&1 | tee \\\n",
    "{model_output_path}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "prostate-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = os.path.join(save_path, \"out\")\n",
    "pathlib.Path(model_output_path).mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "synthetic-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "numerous-freeware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./xlearn_train ./train_test_data/is_vp_succ2/Odia/train.txt -v ./train_test_data/is_vp_succ2/Odia/test.txt -x auc -s 2 -k 32 -m ./train_test_data/is_vp_succ2/Odia/out/model.out -t ./train_test_data/is_vp_succ2/Odia/out/model.txt -b 0.001 --disk 2>&1 | tee ./train_test_data/is_vp_succ2/Odia/out/logs'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "chief-foster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"touch check_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "contemporary-fossil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maximal-furnace-783.rohitrr.train_temp_q0_table_Odia_is_vp_succ2',\n",
       " 'maximal-furnace-783.rohitrr.train_temp_q1_table_Odia_is_vp_succ2',\n",
       " 'maximal-furnace-783.rohitrr.train_temp_q2_table_Odia_is_vp_succ2',\n",
       " 'maximal-furnace-783.rohitrr.train_temp_q3_table_Odia_is_vp_succ2']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_delete_table_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "animal-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "answering-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 10\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "sticky-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train_test_data_models/is_vp_succ2/Kannada/user_post_ffm_mapping.csv\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = \",\")\n",
    "    i = 0\n",
    "    for row in csv_reader:\n",
    "        rows.append(row)\n",
    "        i+=1\n",
    "        if(i > count):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "saved-sunday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_post_1000004482', '1']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "thorough-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./train_test_data_models/is_vp_succ2/Kannada/user_post_ffm_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "similar-optimum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_post_1000004482\\t1\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-genre",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
