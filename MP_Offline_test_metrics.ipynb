{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interracial-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.download_from_GCP import download_table_to_local_as_one_file\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import ndcg_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import pathlib\n",
    "import xlearn as xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noted-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = [\n",
    "    \"Hindi\",\n",
    "    # \"Tamil\",\n",
    "#     \"Telugu\",\n",
    "#     \"Kannada\",\n",
    "#     \"Punjabi\",\n",
    "#     \"Odia\",\n",
    "#     \"Bengali\",\n",
    "#     \"Marathi\",\n",
    "#     \"Malayalam\",\n",
    "#     \"Gujarati\",\n",
    "]\n",
    "# DAYS_OF_DATA_CONSIDERED = 7\n",
    "rating_def_dict = {\n",
    "#         \"vplay\": \"is_vp_succ\",\n",
    "        \"like\": \"is_like\",\n",
    "# #         \"share\": \"is_share\",\n",
    "# #         \"fav\": \"is_fav\",\n",
    "        \"vplay2\": \"is_vp_succ2\",\n",
    "#         \"vplay_skip\": \"is_vp_skip\",\n",
    "#     \"vclick\": \"is_vp_click\"\n",
    "}\n",
    "\n",
    "# table_path = \"maximal-furnace-783.rohitrr.test_temp_q1_table_Malayalam_is_vp_succ2\"\n",
    "# local_save_path = \"./train_test_data_models/Malayalam/is_vp_succ2\"\n",
    "out_file_name = \"test_q1.csv\"\n",
    "min_pos_labels = 10\n",
    "min_total_user_events = 20\n",
    "num_users_to_consider = 20000\n",
    "RANDOM_SEED = 9745\n",
    "TEST_DATA_FILE_NAME = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "another-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proper-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_scores(test_file_path, test_model_path, predicted_output_folder_path, \n",
    "                   rating_def, lang, predicted_output_file_name = \"predicted_output.txt\"):\n",
    "    pathlib.Path(predicted_output_folder_path).mkdir(parents = True, exist_ok = True)\n",
    "    predicted_output_file_path = os.path.join(predicted_output_folder_path,\n",
    "                                             predicted_output_file_name)\n",
    "    res_out_path = os.path.join(predicted_output_folder_path, \"results.csv\")\n",
    "    \n",
    "    print(f\"Predicting labels for {rating_def}-{lang} .... \")\n",
    "    ffm_model = xl.create_ffm() \n",
    "    ffm_model.setTest(test_file_path)\n",
    "    ffm_model.setSigmoid()\n",
    "    ffm_model.predict(test_model_path, predicted_output_file_path)\n",
    "    print(f\"Predicted labels stored in {predicted_output_file_path}\")\n",
    "    \n",
    "def calculate_user_level_metric_scores(true_vals, predicted_vals):\n",
    "        user_at_5_ndcg_score = ndcg_score(true_vals[np.newaxis, :], predicted_vals[np.newaxis, :],\n",
    "                                         k = 5)\n",
    "        user_at_10_ndcg_score = ndcg_score(true_vals[np.newaxis, :], predicted_vals[np.newaxis, :],\n",
    "                                         k = 10)\n",
    "        user_auc_score = roc_auc_score(true_vals, predicted_vals)\n",
    "        relevant_recommendations = np.where(true_vals == 1)[0]\n",
    "        ordered_recommendations = np.argsort(-predicted_vals)\n",
    "        user_mapk_at_5_score = apk(relevant_recommendations, ordered_recommendations, k=5)\n",
    "        user_mapk_at_10_score = apk(relevant_recommendations, ordered_recommendations, k=10)\n",
    "        return [user_at_5_ndcg_score, user_at_10_ndcg_score, \n",
    "                user_auc_score, user_mapk_at_5_score, user_mapk_at_10_score]\n",
    "    #     print(\"appended\")\n",
    "    \n",
    "def get_results(test_file_path, predicted_results_path,\n",
    "                lang, rating_def):\n",
    "    print(\"Getting results for {}-{}\".format(lang, rating_def))\n",
    "    print(f\"Opening {predicted_results_path} and {test_file_path} - reading input ............\")\n",
    "    with open(predicted_results_path) as f:\n",
    "        lines = f.readlines()\n",
    "        predicted_scores = [float(score.replace('\\n','')) for score in lines]\n",
    "    user_mappings = []\n",
    "    scores = []\n",
    "    with open(test_file_path) as f:\n",
    "        for l in f:\n",
    "            user_mappings.append(int(l.split(':')[1]))\n",
    "            scores.append(int(l[0]))\n",
    "    dataframe_dict = {\n",
    "        \"user_mapping\": user_mappings,\n",
    "        \"score\": scores,\n",
    "        \"predicted_score\": predicted_scores\n",
    "    }\n",
    "    print(f\"Calculating overall AUC scores - {lang}-{rating_def}\")\n",
    "    auc_overall = roc_auc_score(scores, predicted_scores)\n",
    "    print(f\"Overall AUC scores computed - {lang}-{rating_def}\")\n",
    "    \n",
    "    df = pd.DataFrame(dataframe_dict)\n",
    "    print(f\"Created test dataframe for {lang}-{rating_def}\")\n",
    "    \n",
    "    print(f\"Separating data into groups {lang}-{rating_def} .....\")\n",
    "    agg_df = df[['user_mapping', 'score']].groupby(['user_mapping']).agg(['sum', 'count'])\n",
    "    agg_df = agg_df[(agg_df[\"score\"][\"sum\"] >= min_pos_labels) \\\n",
    "                     & (agg_df[\"score\"][\"count\"] >= min_total_user_events) \\\n",
    "                   & (agg_df[\"score\"][\"sum\"] != agg_df[\"score\"][\"count\"])]\n",
    "    selected_user_mappings = agg_df.sample(n = num_users_to_consider, replace=False).index.values\n",
    "\n",
    "    df = df[df.user_mapping.isin(selected_user_mappings)]\n",
    "    df = df.sort_values(\"user_mapping\")\n",
    "    all_user_mapping = df.user_mapping.values\n",
    "    scores = df.score.values\n",
    "    predicted_scores = df.predicted_score.values\n",
    "    ukeys, index = np.unique(all_user_mapping, True)\n",
    "    user_level_true_vals = np.split(scores, index[1:])\n",
    "    user_level_predicted_vals = np.split(predicted_scores, index[1:])\n",
    "    input_vals = list(zip(user_level_true_vals, user_level_predicted_vals))\n",
    "    print(f\"Input separated into groups for {lang}-{rating_def}\")\n",
    "    print(f\"Computing different scores for {lang}-{rating_def}\")\n",
    "    with Pool(processes = 48) as sub_pool:\n",
    "        res = sub_pool.starmap(calculate_user_level_metric_scores, input_vals)       \n",
    "    means = np.array(res).mean(axis = 0)\n",
    "    \n",
    "    results_dict = {\n",
    "        \"Rating Definition\": rating_def,\n",
    "        \"Language\": lang,\n",
    "        \"AUC score - Overall\": auc_overall,\n",
    "        \"NDCG@5 Score - User Level\": means[0],\n",
    "        \"NDCG@10 Score - User Level\": means[1],\n",
    "        \"AUC score - User Level\": means[2],\n",
    "        \"MAPK@5 score - User Level\": means[3],\n",
    "        \"MAPK@10 score - User Level\": means[4]\n",
    "        }\n",
    "    print(\"Completed computing results for {} {}\".format(lang, rating_def))\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-scientist",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for Hindi-is_like .... \n",
      "Predicted labels stored in ./train_test_data_models/Hindi/is_like/predicted_results/predicted_scores.txt\n",
      "Getting results for Hindi-is_like\n",
      "Opening ./train_test_data_models/Hindi/is_like/predicted_results/predicted_scores.txt and ./train_test_data_models/Hindi/is_like/test.txt - reading input ............\n",
      "Calculating overall AUC scores - Hindi-is_like\n",
      "Overall AUC scores computed - Hindi-is_like\n",
      "Created test dataframe for Hindi-is_like\n",
      "Separating data into groups Hindi-is_like .....\n",
      "Input separated into groups for Hindi-is_like\n",
      "Computing different scores for Hindi-is_like\n",
      "Completed computing results for Hindi is_like\n",
      "Predicting labels for Hindi-is_vp_succ2 .... \n",
      "Predicted labels stored in ./train_test_data_models/Hindi/is_vp_succ2/predicted_results/predicted_scores.txt\n",
      "Getting results for Hindi-is_vp_succ2\n",
      "Opening ./train_test_data_models/Hindi/is_vp_succ2/predicted_results/predicted_scores.txt and ./train_test_data_models/Hindi/is_vp_succ2/test.txt - reading input ............\n",
      "Calculating overall AUC scores - Hindi-is_vp_succ2\n",
      "Overall AUC scores computed - Hindi-is_vp_succ2\n",
      "Created test dataframe for Hindi-is_vp_succ2\n",
      "Separating data into groups Hindi-is_vp_succ2 .....\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for lang in LANGS:\n",
    "    for rating_key, rating_def in rating_def_dict.items():\n",
    "        test_file_path = f\"./train_test_data_models/{lang}/{rating_def}/{TEST_DATA_FILE_NAME}.txt\"\n",
    "        trained_model_path = f\"./train_test_data_models/{lang}/{rating_def}/out/model.out\"\n",
    "        predicted_output_folder_path = f\"./train_test_data_models/{lang}/{rating_def}/predicted_results\"\n",
    "        predicted_output_file_name = \"predicted_scores.txt\"\n",
    "        predict_scores(test_file_path, trained_model_path, predicted_output_folder_path,\n",
    "                      lang, rating_def, predicted_output_file_name=predicted_output_file_name)\n",
    "        predicted_output_file_path = os.path.join(predicted_output_folder_path, \n",
    "                                                 predicted_output_file_name)\n",
    "        temp_res = get_results(test_file_path, predicted_output_file_path, \n",
    "                                   lang, rating_def)\n",
    "        results.append(temp_res)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "light-satellite",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(results).to_csv(\"results/small_lang_metrics.csv\",index = False, header=False, mode='a')\n",
    "pd.DataFrame(results).to_csv(\"results/referrer_condition_changed_metrics.csv\",index = False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "whole-bridge",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Rating Definition': 'is_like',\n",
       "  'Language': 'Hindi',\n",
       "  'AUC score - Overall': 0.9173405232600731,\n",
       "  'NDCG@5 Score - User Level': 0.5207846852808882,\n",
       "  'NDCG@10 Score - User Level': 0.49515090329355554,\n",
       "  'AUC score - User Level': 0.6961297246491007,\n",
       "  'MAPK@5 score - User Level': 0.4145686666666887,\n",
       "  'MAPK@10 score - User Level': 0.35471615476190893},\n",
       " {'Rating Definition': 'is_vp_succ2',\n",
       "  'Language': 'Hindi',\n",
       "  'AUC score - Overall': 0.8180629133380294,\n",
       "  'NDCG@5 Score - User Level': 0.648882613353443,\n",
       "  'NDCG@10 Score - User Level': 0.6084038451682723,\n",
       "  'AUC score - User Level': 0.7675345083621748,\n",
       "  'MAPK@5 score - User Level': 0.5398481666666789,\n",
       "  'MAPK@10 score - User Level': 0.4596852380952369}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-constitution",
   "metadata": {},
   "source": [
    "### Scrap code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surprised-minimum",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for Malayalam-is_vp_skip\n",
      "Opening ./train_test_data_models/Malayalam/is_vp_skip/predicted_results/predicted_scores.txt and ./train_test_data_models/Malayalam/is_vp_skip/test.txt - reading input ............\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1a6c4851bd43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_results_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mpredicted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0muser_mappings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1a6c4851bd43>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_results_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mpredicted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0muser_mappings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lang in LANGS:\n",
    "    for rating_key, rating_def in rating_def_dict.items():\n",
    "        print(\"Getting results for {}-{}\".format(lang, rating_def))\n",
    "        predicted_results_path = f\"./train_test_data_models/{lang}/{rating_def}/predicted_results/predicted_scores.txt\"\n",
    "        test_file_path = f\"./train_test_data_models/{lang}/{rating_def}/test.txt\"\n",
    "        print(f\"Opening {predicted_results_path} and {test_file_path} - reading input ............\")\n",
    "        with open(predicted_results_path) as f:\n",
    "            lines = f.readlines()\n",
    "            predicted_scores = [float(score.replace('\\n','')) for score in lines]\n",
    "        user_mappings = []\n",
    "        scores = []\n",
    "        with open(test_file_path) as f:\n",
    "            for l in f:\n",
    "                user_mappings.append(int(l.split(':')[1]))\n",
    "                scores.append(int(l[0]))\n",
    "        dataframe_dict = {\n",
    "            \"user_mapping\": user_mappings,\n",
    "            \"score\": scores,\n",
    "            \"predicted_score\": predicted_scores\n",
    "        }\n",
    "        df = pd.DataFrame(dataframe_dict)\n",
    "        print(f\"Created test dataframe for {lang}-{rating_def}\")\n",
    "\n",
    "        print(f\"Separating data into groups {lang}-{rating_def} .....\")\n",
    "        agg_df = df[['user_mapping', 'score']].groupby(['user_mapping']).agg(['sum', 'count'])\n",
    "        agg_df = agg_df[(agg_df[\"score\"][\"sum\"] >= min_pos_labels) \\\n",
    "                         & (agg_df[\"score\"][\"count\"] >= min_total_user_events) \\\n",
    "                       & (agg_df[\"score\"][\"sum\"] != agg_df[\"score\"][\"count\"])]\n",
    "        selected_user_mappings = agg_df.sample(n = num_users_to_consider, replace=False).index.values\n",
    "\n",
    "        df = df[df.user_mapping.isin(selected_user_mappings)]\n",
    "        df = df.sort_values(\"user_mapping\")\n",
    "        all_user_mapping = df.user_mapping.values\n",
    "        scores = df.score.values\n",
    "        predicted_scores = df.predicted_score.values\n",
    "        ukeys, index = np.unique(all_user_mapping, True)\n",
    "        user_level_true_vals = np.split(scores, index[1:])\n",
    "        user_level_predicted_vals = np.split(predicted_scores, index[1:])\n",
    "        input_vals = list(zip(user_level_true_vals, user_level_predicted_vals))\n",
    "        print(f\"Input separated into groups for {lang}-{rating_def}\")\n",
    "        print(f\"Computing scores for {lang}-{rating_def}\")\n",
    "    #     with Pool(processes = 24) as sub_pool:\n",
    "    #         res = sub_pool.starmap(calculate_metric_scores, input_vals)\n",
    "        res = []\n",
    "        for true_vals, predicted_vals in tqdm(zip(user_level_true_vals, user_level_predicted_vals)):\n",
    "#             print(true_vals, predicted_vals)\n",
    "            temp_res = calculate_user_level_metric_scores(true_vals, predicted_vals)\n",
    "            res.append(temp_res)\n",
    "\n",
    "        means = np.array(res).mean(axis = 0)\n",
    "\n",
    "        results_dict = {\n",
    "            \"Rating Definition\": rating_key,\n",
    "            \"Language\": lang,\n",
    "            \"NDCG Score - User Level\": means[0],\n",
    "            \"AUC score - User Level\": means[1],\n",
    "            \"MAPK score - User Level\": means[2]\n",
    "            }\n",
    "        print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divided-egypt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for Hindi-is_vp_succ2\n",
      "Getting results for Hindi-is_vp_succ2\n",
      "Opening ./train_test_data_models/Hindi/is_vp_succ2/predicted_results/predicted_scores.txt and ./train_test_data_models/Hindi/is_vp_succ2/test.txt - reading input ............\n",
      "Calculating overall AUC scores - Hindi-is_vp_succ2\n",
      "Overall AUC scores computed - Hindi-is_vp_succ2\n",
      "Created test dataframe for Hindi-is_vp_succ2\n",
      "Separating data into groups Hindi-is_vp_succ2 .....\n",
      "Input separated into groups for Hindi-is_vp_succ2\n",
      "Computing scores for Hindi-is_vp_succ2\n",
      "Completed computing results for Hindi is_vp_succ2\n",
      "{'Rating Definition': 'is_vp_succ2', 'Language': 'Hindi', 'AUC score - Overall': 0.8094117656071651, 'NDCG@5 Score - User Level': 0.6335483608648548, 'NDCG@10 Score - User Level': 0.5937726673437211, 'AUC score - User Level': 0.7559612853287495, 'MAPK@5 score - User Level': 0.5233338333333528, 'MAPK@10 score - User Level': 0.4437434563492078}\n",
      "Getting results for Hindi-is_vp_skip\n",
      "Getting results for Hindi-is_vp_skip\n",
      "Opening ./train_test_data_models/Hindi/is_vp_skip/predicted_results/predicted_scores.txt and ./train_test_data_models/Hindi/is_vp_skip/test.txt - reading input ............\n",
      "Calculating overall AUC scores - Hindi-is_vp_skip\n",
      "Overall AUC scores computed - Hindi-is_vp_skip\n",
      "Created test dataframe for Hindi-is_vp_skip\n",
      "Separating data into groups Hindi-is_vp_skip .....\n",
      "Input separated into groups for Hindi-is_vp_skip\n",
      "Computing scores for Hindi-is_vp_skip\n",
      "Completed computing results for Hindi is_vp_skip\n",
      "{'Rating Definition': 'is_vp_skip', 'Language': 'Hindi', 'AUC score - Overall': 0.7397915548064928, 'NDCG@5 Score - User Level': 0.7830978439765075, 'NDCG@10 Score - User Level': 0.7154560320564455, 'AUC score - User Level': 0.6828310436971734, 'MAPK@5 score - User Level': 0.7093703333333223, 'MAPK@10 score - User Level': 0.5912715972222189}\n"
     ]
    }
   ],
   "source": [
    "for lang in LANGS:\n",
    "    for rating_key, rating_def in rating_def_dict.items():\n",
    "        print(\"Getting results for {}-{}\".format(lang, rating_def))\n",
    "        predicted_results_path = f\"./train_test_data_models/{lang}/{rating_def}/predicted_results/predicted_scores.txt\"\n",
    "#         trained_model_path = f\"./train_test_data_models/{lang}/{rating_def}/out/model.out\"\n",
    "#         predicted_output_folder_path = f\"./train_test_data_models/{lang}/{rating_def}/predicted_results\"\n",
    "#         predicted_output_file_name = \"predicted_scores.txt\"\n",
    "        test_file_path = f\"./train_test_data_models/{lang}/{rating_def}/test.txt\"\n",
    "#         predicted_results_path = f\"./train_test_data_models/{lang}/{rating_def}/predicted_results/predicted_scores.txt\"\n",
    "#         predict_scores(test_file_path, trained_model_path, predicted_output_folder_path,\n",
    "#                       lang, rating_def, predicted_output_file_name=predicted_output_file_name)\n",
    "#         predicted_results_path = os.path.join(predicted_output_folder_path, predicted_output_file_name)\n",
    "        \n",
    "        test_file_path = f\"./train_test_data_models/{lang}/{rating_def}/test.txt\"\n",
    "        print(\"Getting results for {}-{}\".format(lang, rating_def))\n",
    "        print(f\"Opening {predicted_results_path} and {test_file_path} - reading input ............\")\n",
    "        with open(predicted_results_path) as f:\n",
    "            lines = f.readlines()\n",
    "            predicted_scores = [float(score.replace('\\n','')) for score in lines]\n",
    "        user_mappings = []\n",
    "        scores = []\n",
    "        with open(test_file_path) as f:\n",
    "            for l in f:\n",
    "                user_mappings.append(int(l.split(':')[1]))\n",
    "                scores.append(int(l[0]))\n",
    "        dataframe_dict = {\n",
    "            \"user_mapping\": user_mappings,\n",
    "            \"score\": scores,\n",
    "            \"predicted_score\": predicted_scores\n",
    "        }\n",
    "        print(f\"Calculating overall AUC scores - {lang}-{rating_def}\")\n",
    "        auc_overall = roc_auc_score(scores, predicted_scores)\n",
    "        print(f\"Overall AUC scores computed - {lang}-{rating_def}\")\n",
    "\n",
    "        df = pd.DataFrame(dataframe_dict)\n",
    "        print(f\"Created test dataframe for {lang}-{rating_def}\")\n",
    "\n",
    "        print(f\"Separating data into groups {lang}-{rating_def} .....\")\n",
    "        agg_df = df[['user_mapping', 'score']].groupby(['user_mapping']).agg(['sum', 'count'])\n",
    "        agg_df = agg_df[(agg_df[\"score\"][\"sum\"] >= min_pos_labels) \\\n",
    "                         & (agg_df[\"score\"][\"count\"] >= min_total_user_events) \\\n",
    "                       & (agg_df[\"score\"][\"sum\"] != agg_df[\"score\"][\"count\"])]\n",
    "        selected_user_mappings = agg_df.sample(n = num_users_to_consider, replace=False).index.values\n",
    "\n",
    "        df = df[df.user_mapping.isin(selected_user_mappings)]\n",
    "        df = df.sort_values(\"user_mapping\")\n",
    "        all_user_mapping = df.user_mapping.values\n",
    "        scores = df.score.values\n",
    "        predicted_scores = df.predicted_score.values\n",
    "        ukeys, index = np.unique(all_user_mapping, True)\n",
    "        user_level_true_vals = np.split(scores, index[1:])\n",
    "        user_level_predicted_vals = np.split(predicted_scores, index[1:])\n",
    "        input_vals = list(zip(user_level_true_vals, user_level_predicted_vals))\n",
    "        print(f\"Input separated into groups for {lang}-{rating_def}\")\n",
    "        print(f\"Computing scores for {lang}-{rating_def}\")\n",
    "        with Pool(processes = 48) as sub_pool:\n",
    "            res = sub_pool.starmap(calculate_user_level_metric_scores, input_vals)\n",
    "#             res = []\n",
    "#             for true_vals, predicted_vals in tqdm(zip(user_level_true_vals, user_level_predicted_vals)):\n",
    "#         #             print(true_vals, predicted_vals)\n",
    "#                 temp_res = calculate_user_level_metric_scores(true_vals, predicted_vals)\n",
    "#                 res.append(temp_res)        \n",
    "        means = np.array(res).mean(axis = 0)\n",
    "\n",
    "        results_dict = {\n",
    "            \"Rating Definition\": rating_def,\n",
    "            \"Language\": lang,\n",
    "            \"AUC score - Overall\": auc_overall,\n",
    "            \"NDCG@5 Score - User Level\": means[0],\n",
    "            \"NDCG@10 Score - User Level\": means[1],\n",
    "            \"AUC score - User Level\": means[2],\n",
    "            \"MAPK@5 score - User Level\": means[3],\n",
    "            \"MAPK@10 score - User Level\": means[4]\n",
    "            }\n",
    "        print(\"Completed computing results for {} {}\".format(lang, rating_def))\n",
    "        print(results_dict)\n",
    "        df = pd.DataFrame([results_dict])\n",
    "        df.to_csv(\"./results/Hindi_rest_res_1.csv\", mode='a', \n",
    "                  index=False, header=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sitting-islam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rating Definition': 'is_vp_skip',\n",
       " 'Language': 'Malayalam',\n",
       " 'AUC score - Overall': 0.7403779265112256,\n",
       " 'NDCG@5 Score - User Level': 0.7511589330745353,\n",
       " 'NDCG@10 Score - User Level': 0.714626302675468,\n",
       " 'AUC score - User Level': 0.6773018945198868,\n",
       " 'MAPK@5 score - User Level': 0.6617723333333213,\n",
       " 'MAPK@10 score - User Level': 0.5869704742063461}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-syria",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"./aggregated_results/other_metrics_1.csv\", mode='a', \n",
    "          index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rough-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495644647, 769884558)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_scores), len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "worldwide-experiment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.1 ms, sys: 19.9 ms, total: 67 ms\n",
      "Wall time: 65.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "user_mapping_df = df[df.user_mapping == user_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "periodic-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2447441"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.predicted_score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "spoken-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[df.user_mapping.isin(selected_user_mappings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "square-actor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_df = df[df.userId.isin(selected_userIds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "metric-connectivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2447441, 2447441, 62711999)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(\"user_mapping\")\n",
    "all_user_mapping = df.user_mapping.values\n",
    "scores = df.score.values\n",
    "precicted_scores = df.predicted_score.values\n",
    "len(all_user_mapping), len(scores), len(predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "maritime-irrigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_mapping</th>\n",
       "      <th>score</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7782684</th>\n",
       "      <td>68939</td>\n",
       "      <td>0</td>\n",
       "      <td>0.788298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032797</th>\n",
       "      <td>68939</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017182</th>\n",
       "      <td>68939</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12573382</th>\n",
       "      <td>68939</td>\n",
       "      <td>0</td>\n",
       "      <td>0.313040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53359825</th>\n",
       "      <td>68939</td>\n",
       "      <td>0</td>\n",
       "      <td>0.577004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_mapping  score  predicted_score\n",
       "7782684          68939      0         0.788298\n",
       "7032797          68939      0         0.687772\n",
       "7017182          68939      1         0.500768\n",
       "12573382         68939      0         0.313040\n",
       "53359825         68939      0         0.577004"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "random-bargain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>tagId</th>\n",
       "      <th>score</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9857235382</th>\n",
       "      <td>119103269175624</td>\n",
       "      <td>199314537408</td>\n",
       "      <td>77330</td>\n",
       "      <td>25836.966834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365384382</th>\n",
       "      <td>116716063066117</td>\n",
       "      <td>195051641932</td>\n",
       "      <td>72874</td>\n",
       "      <td>25429.166583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9329535382</th>\n",
       "      <td>106995824611773</td>\n",
       "      <td>178190688536</td>\n",
       "      <td>70690</td>\n",
       "      <td>23113.308994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9062058282</th>\n",
       "      <td>98212164199390</td>\n",
       "      <td>163874211312</td>\n",
       "      <td>68897</td>\n",
       "      <td>21243.150008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372802382</th>\n",
       "      <td>90735147065879</td>\n",
       "      <td>152006896006</td>\n",
       "      <td>68039</td>\n",
       "      <td>19701.435099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     userId         tagId  score  predicted_score\n",
       "postId                                                           \n",
       "9857235382  119103269175624  199314537408  77330     25836.966834\n",
       "3365384382  116716063066117  195051641932  72874     25429.166583\n",
       "9329535382  106995824611773  178190688536  70690     23113.308994\n",
       "9062058282   98212164199390  163874211312  68897     21243.150008\n",
       "1372802382   90735147065879  152006896006  68039     19701.435099"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['postId']).sum().sort_values(by=['score'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "unknown-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df[['userId', 'score']].groupby(['userId']).agg(['sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "handy-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55914986, 5)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "billion-relative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([('score',   'sum'),\n",
       "            ('score', 'count')],\n",
       "           )"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "broadband-pottery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     53871,      55059,      55062,      55564,      56773,\n",
       "                 57106,      57358,      57549,      58644,      58785,\n",
       "            ...\n",
       "            2777448214, 2777458053, 2777461328, 2777461595, 2777473212,\n",
       "            2777478170, 2777479686, 2777479756, 2777481430, 2777483046],\n",
       "           dtype='int64', name='userId', length=1125986)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "personalized-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  3, 22, ...,  1,  3,  1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df[('score', 'sum')].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "european-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = agg_df[(agg_df[\"score\"][\"sum\"] >= min_pos_labels) & (agg_df[\"score\"][\"count\"] >= min_total_user_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "joint-parts",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'userId'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'userId'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-2d12a67e8d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"userId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3024\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_multilevel\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m         \u001b[0;31m# self.columns is a MultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3074\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3075\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m             \u001b[0mnew_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method)\u001b[0m\n\u001b[1;32m   2874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2876\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2877\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_maybe_to_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc_single_level_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexsort_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_loc_single_level_index\u001b[0;34m(self, level_index, key)\u001b[0m\n\u001b[1;32m   2807\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlevel_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'userId'"
     ]
    }
   ],
   "source": [
    "temp_df[\"userId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "advised-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = agg_df.sample(n = 20000, replace=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "imperial-lawyer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 176951320, 2618125270, 2580975920,  323434670,  352372614])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "unlimited-stand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>postId</th>\n",
       "      <th>tagId</th>\n",
       "      <th>score</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1114791014</td>\n",
       "      <td>7031720382</td>\n",
       "      <td>102871</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>453980094</td>\n",
       "      <td>5351672082</td>\n",
       "      <td>830883</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2738524875</td>\n",
       "      <td>5325077382</td>\n",
       "      <td>3464496</td>\n",
       "      <td>0</td>\n",
       "      <td>0.144696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53693143</td>\n",
       "      <td>9836841382</td>\n",
       "      <td>1381366</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618968903</td>\n",
       "      <td>5837607382</td>\n",
       "      <td>4474183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId      postId    tagId  score  predicted_score\n",
       "0  1114791014  7031720382   102871      0         0.031885\n",
       "1   453980094  5351672082   830883      0         0.023767\n",
       "2  2738524875  5325077382  3464496      0         0.144696\n",
       "3    53693143  9836841382  1381366      1         0.044900\n",
       "4   618968903  5837607382  4474183      0         0.034953"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "searching-archive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>postId</th>\n",
       "      <th>tagId</th>\n",
       "      <th>score</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46573</th>\n",
       "      <td>2618125270</td>\n",
       "      <td>7919181382</td>\n",
       "      <td>1381366</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86354</th>\n",
       "      <td>2618125270</td>\n",
       "      <td>7967645182</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.128524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88520</th>\n",
       "      <td>352372614</td>\n",
       "      <td>7212202382</td>\n",
       "      <td>1272240</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126024</th>\n",
       "      <td>2618125270</td>\n",
       "      <td>7981172182</td>\n",
       "      <td>9777582</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207918</th>\n",
       "      <td>352372614</td>\n",
       "      <td>1458832382</td>\n",
       "      <td>1381366</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55210329</th>\n",
       "      <td>352372614</td>\n",
       "      <td>1491271182</td>\n",
       "      <td>8291806</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55339373</th>\n",
       "      <td>176951320</td>\n",
       "      <td>7423722282</td>\n",
       "      <td>711045</td>\n",
       "      <td>0</td>\n",
       "      <td>0.826163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55607095</th>\n",
       "      <td>352372614</td>\n",
       "      <td>7217636382</td>\n",
       "      <td>1272240</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55723968</th>\n",
       "      <td>2618125270</td>\n",
       "      <td>3287529082</td>\n",
       "      <td>102871</td>\n",
       "      <td>0</td>\n",
       "      <td>0.182957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55885142</th>\n",
       "      <td>352372614</td>\n",
       "      <td>1726491382</td>\n",
       "      <td>894564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              userId      postId    tagId  score  predicted_score\n",
       "46573     2618125270  7919181382  1381366      1         0.278692\n",
       "86354     2618125270  7967645182       74      0         0.128524\n",
       "88520      352372614  7212202382  1272240      0         0.023171\n",
       "126024    2618125270  7981172182  9777582      0         0.143801\n",
       "207918     352372614  1458832382  1381366      0         0.022506\n",
       "...              ...         ...      ...    ...              ...\n",
       "55210329   352372614  1491271182  8291806      0         0.080947\n",
       "55339373   176951320  7423722282   711045      0         0.826163\n",
       "55607095   352372614  7217636382  1272240      1         0.078268\n",
       "55723968  2618125270  3287529082   102871      0         0.182957\n",
       "55885142   352372614  1726491382   894564      0         0.059187\n",
       "\n",
       "[508 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.userId.isin(t.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "constitutional-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df.userId == 176951320].sort_values(by = ['predicted_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "upper-bouquet",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-fef801da2745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mndcg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0m_check_dcg_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m     \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ndcg_sample_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_ties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_ties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.8/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_check_dcg_target_type\u001b[0;34m(y_true)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                      \"multiclass-multioutput\")\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_fmt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1308\u001b[0m             \"Only {} formats are supported. Got {} instead\".format(\n\u001b[1;32m   1309\u001b[0m                 supported_fmt, y_type))\n",
      "\u001b[0;31mValueError\u001b[0m: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead"
     ]
    }
   ],
   "source": [
    "ndcg_score(temp.score.values, temp.predicted_score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "potential-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vals = np.asarray([temp.score.values])\n",
    "pr_vals = np.asarray([temp.predicted_score.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "postal-lightning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7520911855422138"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_score(tr_vals,pr_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "immune-novelty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_score(np.array([[3,1,1,0,0]]), np.array([[2,1,0.2,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dedicated-salon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(tr_vals[0], pr_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "authentic-rating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "painful-characteristic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33999999999999997"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_recommendations = np.where(tr_vals[0] == 1)[0]\n",
    "ordered_recommendations = np.argsort(-pr_vals[0])\n",
    "apk(relevant_recommendations, ordered_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "general-framing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  3,  8, 12])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "loaded-content",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "extended-davis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "arctic-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./train_test_data_models/Malayalam/is_vp_succ2/test_later.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "incident-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path) as f:\n",
    "    lines = f.readlines()\n",
    "    labels = [int(l.replace('\\n', '')[0]) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "sound-declaration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "material-legislature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0318848, 0.0237671, 0.144696, 0.0449003, 0.0349526]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "still-civilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8400228558390365"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(labels, predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "through-palmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 0:347890:1 1:1475:1 \\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "corrected-magazine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 0', '347890', '1 1', '1475', '1 \\n']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0].split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "disabled-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    \"c1\": [1,2,2,3,2,2,4,5,5,9],\n",
    "    \"c2\": [1,0,0,0,1,1,1,0,1,0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "responsible-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "incident-drain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c1  c2\n",
       "0   1   1\n",
       "1   2   0\n",
       "2   2   0\n",
       "3   3   0\n",
       "4   2   1\n",
       "5   2   1\n",
       "6   4   1"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.groupby(\"c1\").filter(lambda x: x.c1.max() < 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "embedded-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_group(x, selected_user_mappings):\n",
    "    if x.user_mapping in selected_user_mappings:\n",
    "        true_vals = x.score.values\n",
    "        predicted_vals = x.predicted_score.values\n",
    "        print(true_vals, predicted_vals)\n",
    "        user_ndcg_score = ndcg_score(true_vals[np.newaxis, :], predicted_vals[np.newaxis, :])\n",
    "        relevant_recommendations = np.where(true_vals == 1)[0]\n",
    "        ordered_recommendations = np.argsort(-predicted_vals)\n",
    "        user_apk_score = apk(relevant_recommendations, ordered_recommendations)\n",
    "        return (user_ndcg_score, user_auc_score, user_apk_score)\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_1(p1, p2):\n",
    "    p1 = p1[0]\n",
    "    p2 = p2[0]\n",
    "    return [p1+p1*p2, p1-p1*p2]\n",
    "params = [([1],[2]),([2],[3]),[[3],[4]],[[4],[5]]]\n",
    "with Pool(processes=10) as pool:\n",
    "    res = pool.starmap(mp_1, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dominant-august",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, -1], [8, -4], [15, -9], [24, -16]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "supported-mounting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, -1], [8, -4], [15, -9], [24, -16]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extra-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.arange(3), np.arange(3)+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confirmed-cartridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([2, 3, 4])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "southeast-input",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "published-father",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(l).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "strange-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    \"c1\": [1,2,2,3,2,2,4,5,5,9],\n",
    "    \"c2\": [1,0,0,0,1,1,1,0,1,0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "finnish-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "composite-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = df.sort_values('c1').c2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ordered-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.sort_values('c1').c1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sticky-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = np.unique(temp, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unauthorized-breathing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1]),\n",
       " array([0, 0, 1, 1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0, 1]),\n",
       " array([0])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(temp_1, index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "temporal-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c1  c2\n",
       "0   1   1\n",
       "1   2   0\n",
       "2   2   0\n",
       "4   2   1\n",
       "5   2   1\n",
       "3   3   0\n",
       "6   4   1\n",
       "7   5   0\n",
       "8   5   1\n",
       "9   9   0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "major-forest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9469024295259745"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "from math import log\n",
    "# we have groud-truth relevance of some answers to a query:\n",
    "true_relevance = np.asarray([[1, 1, 0, 0, 1, 0]])\n",
    "# we predict some scores (relevance) for the answers\n",
    "scores = np.asarray([[.9, .85, .8, .7, .65, 0.15]])\n",
    "ndcg_score(true_relevance, scores)\n",
    "\n",
    "# scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n",
    "# ndcg_score(true_relevance, scores)\n",
    "\n",
    "# we can set k to truncate the sum; only top k answers contribute.\n",
    "\n",
    "\n",
    "# the normalization takes k into account so a perfect answer\n",
    "# would still get 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "attempted-presentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7653606369886218"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_score(true_relevance, scores, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "disciplinary-offense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_relevance = np.asarray([[1, 1, 0, 0]])\n",
    "# we predict some scores (relevance) for the answers\n",
    "scores = np.asarray([[.9, .85, .8, .7]])\n",
    "ndcg_score(true_relevance, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "earned-employee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1309297535714573"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/log(2,2)+1/log(3,2)+1/log(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "greater-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6309297535714573"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/log(2,2)+1/log(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "reported-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7652582159624413"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.63/2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-tournament",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1bcefc3e94b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train_test_data_models/Kannada/is_vp_succ2/user_post_ffm_mapping.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "temp_df = pd.read_csv(\"./train_test_data_models/Kannada/is_vp_succ2/user_post_ffm_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-phase",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
